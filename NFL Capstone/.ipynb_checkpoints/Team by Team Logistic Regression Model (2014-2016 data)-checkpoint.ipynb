{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_file_path(year):\n",
    "    #generates the file path to csv file that contains annual play by play data in the NFL\n",
    "    file_path = 'C:\\\\Users\\\\abird\\\\Documents\\\\Springboard\\\\Capstone Project\\\\NFL\\\\pbp-'\n",
    "    year = str(year)\n",
    "    return file_path + year + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    #loads csv file into data frame and returns the data frame\n",
    "    df = pd.DataFrame.from_csv(filePath)\n",
    "    #unindex GameId\n",
    "    df = df.reset_index('GameId')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_teams_lst(df):\n",
    "    #stores all the team abbreviation\n",
    "    teams = df.OffenseTeam.dropna().drop_duplicates().sort_values()\n",
    "    return list(teams)\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    '''This function will apply preprocessing steps to the initial data frame'''\n",
    "    \n",
    "    #sort the data frame so all games are in order\n",
    "    df = df.sort_values(by = ['GameId', 'Quarter', 'Minute', 'Second'], ascending = [True, True, False, False])\n",
    "    \n",
    "    #we only want plays that are RUSH or PASS for purposes of prediction\n",
    "    df_subset = df[np.logical_or(df.PlayType == 'RUSH', df.PlayType == 'PASS')]\n",
    "    \n",
    "    #subset of the data that we believe will be important for our prediction problem\n",
    "    df_final = df_subset[['OffenseTeam', 'DefenseTeam', 'Down', 'ToGo'\n",
    "                      , 'YardLine', 'YardLineDirection', 'Quarter', 'Minute', 'Second','Formation', 'PlayType']]\n",
    "    \n",
    "    #get rid of rows that have NaN's in any columns\n",
    "    df_final = df_final.dropna()\n",
    "    \n",
    "    #create a label encoding column in the dataframe for PlayType since it is the variable we are trying to predict\n",
    "    df_final.PlayType = df_final.PlayType.astype('category')\n",
    "    df_final['PlayType_cat'] = df_final.PlayType.cat.codes\n",
    "    \n",
    "    #create dummy variables for each feature that is categorical\n",
    "    #(OffenseTeam, DefenseTeam, YardLineDirection, and Formation)\n",
    "    df_final = pd.get_dummies(df_final, columns = ['OffenseTeam', 'DefenseTeam', 'YardLineDirection', 'Formation'])\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "\n",
    "def subset_team(df, team):\n",
    "    '''This function takes in a data frame of play by play data and a str abbreviation for a team and\n",
    "    returns the subset of plays belonging to that specific team'''\n",
    "    subset_df = df[df.OffenseTeam == team]\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "def round_array(array):\n",
    "    '''return a list of floats that have been rounded from given array'''\n",
    "    lst_to_return = []\n",
    "    for entry in array:\n",
    "        lst_to_return.append(round(entry))\n",
    "    return lst_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years_of_data_lst = [2014, 2015, 2016] #years of available data\n",
    "\n",
    "Cs = [0.001, 0.1, 1, 10, 100] #regularization constants\n",
    "\n",
    "headers = [\n",
    "    'year',\n",
    "    'team',\n",
    "    'acc_score_train_set',\n",
    "    'acc_score_test_set',\n",
    "    'cross_val_score',\n",
    "    'best_reg_par',\n",
    "    'acc_score_best_reg',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'f1_score',\n",
    "    'auc'\n",
    "]\n",
    "\n",
    "model_results_lst = [] #list that stores each team's model results for each year of data\n",
    "\n",
    "#store model results in csv file for further analysis\n",
    "csv_file_path = 'C:\\\\Users\\\\abird\\\\Documents\\\\Springboard\\\\Capstone Project\\\\NFL\\\\team_logistic_reg_results.csv'\n",
    "\n",
    "#start loop to generate team model for each year and gather model evaluation metrics\n",
    "for year in years_of_data_lst:\n",
    "    filePath = generate_file_path(year)\n",
    "    df = load_data(filePath)\n",
    "    teams_lst = generate_teams_lst(df)\n",
    "\n",
    "\n",
    "    #start building a regularized logistic regression model for each team\n",
    "    for team in teams_lst:\n",
    "        #subset of data for each specific team\n",
    "        subset_df = subset_team(df, team)\n",
    "\n",
    "        #preprocess subset_df\n",
    "        df_final = preprocess_data(subset_df)\n",
    "\n",
    "        #let's create a train/test set\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_final.drop(labels = ['PlayType', 'PlayType_cat'], axis = 1).values, \n",
    "                                                  df_final.PlayType_cat.values, random_state = 69)\n",
    "\n",
    "        #let's fit a logistic regression model to predict PlayType (run or pass)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc_score_train_set = accuracy_score(clf.predict(x_train), y_train)\n",
    "        acc_score_test_set = accuracy_score(clf.predict(x_test), y_test)\n",
    "        #print team\n",
    "        #print 'Accuracy score on training set: ', acc_score_train_set\n",
    "        #print 'Accuracy score on test set: ', acc_score_test_set\n",
    "\n",
    "        #5-Fold Cross Validation on training set\n",
    "        cross_val_score = cv_score(clf, x_train, y_train)\n",
    "        #print 'cv_score: ', cross_val_score\n",
    "\n",
    "        #Grid Search for the best regularization constant for logistic regression\n",
    "        param_grid = {'C' : Cs }\n",
    "        model = GridSearchCV(LogisticRegression(), param_grid, scoring = 'accuracy', cv = 5)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        best_reg_par = model.best_params_['C']\n",
    "        acc_score_best_reg = accuracy_score(model.predict(x_test), y_test)\n",
    "        #print 'The best regularization parameter from the Grid Search is:', best_reg_par\n",
    "        #print 'The accuracy score with the best regularization parameter is', acc_score_best_reg\n",
    "\n",
    "        #AUC score for the ROC Curve for each team based model\n",
    "        preds = model.predict_proba(x_test)[:,1]\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n",
    "\n",
    "        y_test_rounded = round_array(y_test)\n",
    "        preds_rounded = round_array(preds)\n",
    "\n",
    "        #precision (true positives / [true positives + false positives])\n",
    "        precision = metrics.precision_score(y_test_rounded, preds_rounded)\n",
    "\n",
    "        #recall (true positives / [true positives + false negatives])\n",
    "        recall = metrics.recall_score(y_test_rounded, preds_rounded)\n",
    "\n",
    "        #F1 score is the geometric/harmonic mean of precision and recall; 2 * tp / ( 2 * tp + fp + fn)\n",
    "        f1_score = metrics.f1_score(y_test_rounded, preds_rounded)\n",
    "        \n",
    "        #AUC (area under the curve)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        #print 'The precision score is', precision\n",
    "        #print 'The recall score is', recall\n",
    "        #print 'The F1 score is', f1_score\n",
    "        #print 'The AUC for the ROC Curve is', auc\n",
    "        #print '\\n'\n",
    "\n",
    "        tup = (\n",
    "        year,\n",
    "        team,\n",
    "        acc_score_train_set,\n",
    "        acc_score_test_set,\n",
    "        cross_val_score,\n",
    "        best_reg_par,\n",
    "        acc_score_best_reg,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        auc\n",
    "        )\n",
    "        model_results_lst.append(tup)\n",
    "\n",
    "model_results_df = pd.DataFrame(model_results_lst, columns = headers)\n",
    "model_results_df = model_results_df.sort_values(by = ['year', 'team'], ascending = [True, True])\n",
    "model_results_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
